{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as  np\n",
    "import matplotlib.pyplot as plt\n",
    "import lib\n",
    "from collections import defaultdict        \n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Q1& Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = 0.5\n",
    "nb_coins = 1000\n",
    "nb_times = 10\n",
    "nb_repeats = 100000\n",
    "\n",
    "samples = np.random.uniform(size=(nb_repeats, nb_coins, nb_times)) <= mu\n",
    "s = samples.mean(axis=2)\n",
    "\n",
    "v1 = s[:, 0]\n",
    "vrand = s[np.arange(nb_repeats), np.random.randint(0, nb_coins, size=nb_repeats)]\n",
    "vmin = s.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(vrand.mean())\n",
    "print(vmin.mean())\n",
    "print(v1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 :  closest is b\n",
    "\n",
    "## Q2 : c1 and crand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "modelisation of the problem\n",
    "\n",
    "- $h$ is a random variable denoting our hypothesis function.\n",
    "- $f$ is a random variable denoting our real function.\n",
    "- $f'$ is a random variable denoting the noisified version of the real function\n",
    "\n",
    "We have:\n",
    "\n",
    "- $P(h=1|f=1) = 1 - \\mu$\n",
    "- $P(h=0|f=0) = 1 - \\mu$\n",
    "- $P(f'=1|f=1) = \\lambda$\n",
    "- $P(f'=0|f=0) = \\lambda$\n",
    "\n",
    "we want to find : $P(h \\neq f') = P(h=1, f'=0) + P(h=1, f'=1)$\n",
    "\n",
    "just to verify for $P(h \\neq f), which is equal to $\\mu$\n",
    "\n",
    "$P(h \\neq f) = P(h=1, f=0) + P(h=0, f=1) = P(h=1|f=0)P(f=0) + P(h=0|f=1)P(f=1) = \\frac{1}{2}\\mu + \\frac{1}{2}\\mu = \\mu$\n",
    "\n",
    "now $P(h \\neq f')$.\n",
    "\n",
    "\n",
    "$P(h \\neq f') = P(h=1, f'=0) + P(h=0, f'=1)$\n",
    "\n",
    "$= P(h=1, f'=0|f=0)P(f=0) + P(h=1, f'=0|f=1)P(f=1) + P(h=0, f'=1|f=0)P(f=0) +  P(h=0, f'=1|f=1)P(f=1) $\n",
    "\n",
    "$= P(h=1|f=0)P(f'=0|f=0)P(f=0)+ P(h=1|f=1)P(f'=0|f=1)P(f=1)+ P(h=0|f=0)P(f'=1|f=0)P(f=1) + P(h=0|f=1)P(f'=1|f=1)P(f=1)$\n",
    "\n",
    "$ = \\frac{1}{2}\\mu \\lambda + \\frac{1}{2}(1-\\mu)(1-\\lambda) + \\frac{1}{2}\\mu \\lambda + \\frac{1}{2})(1-\\mu)(1-\\lambda)$\n",
    "\n",
    "$= \\mu \\lambda + (1-\\mu)(1-\\lambda)$\n",
    "\n",
    "## Q4\n",
    "\n",
    "well if $\\lambda = 0.5$ then $\\mu$ cancels, b is the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## q3 verification\n",
    "size = 100000\n",
    "x = np.random.uniform(size=size) <= 0.6\n",
    "f = (x > 0.4)\n",
    "\n",
    "mu = 0.3\n",
    "r = np.random.uniform(size=size) <= (1 - mu)\n",
    "h = f * r + (1 - f) * (1 - r)\n",
    "\n",
    "print((h!=f).mean())\n",
    "\n",
    "lambda_ = 0.1\n",
    "r = np.random.uniform(size=size)<= (lambda_)\n",
    "fprime = f * r + (1 - f) * (1 - r)\n",
    "print((fprime != f).mean())\n",
    "print((fprime != h).mean(), mu*lambda_ + (1-mu)*(1-lambda_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(nb=10):\n",
    "    x1, y1, x2, y2 = np.random.uniform(-1, 1, size=4)\n",
    "    m = (y2-y1)/(x2-x1)\n",
    "    p = y1 - m * x1\n",
    "    x_train = np.random.uniform(-1, 1, size=(nb, 2))\n",
    "    a = m\n",
    "    b = -1\n",
    "    c = p\n",
    "    w = np.array([a, b])\n",
    "    y_train = (np.dot(x_train, w) + c > 0) * 2 - 1\n",
    "    return x_train, y_train\n",
    "\n",
    "def get_line_y(x1, x2, a, b, c):\n",
    "    # get y1 and y2 of corresponding x1 and x2 passing through the line defined by (ax+by+c=0)\n",
    "    y1 = (- c - a * x1) / b\n",
    "    y2 = (- c - a * x2) / b\n",
    "    return y1, y2\n",
    "\n",
    "\n",
    "def viz(x_train, y_train, w, fig):\n",
    "    x1, x2 = x_train[:, 0].min(), x_train[:, 0].max()\n",
    "    y1, y2 = get_y(x1, x2, w[0], w[1], w[2])\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(x_train[:, 0], x_train[:, 1], c=y_train)\n",
    "    plt.plot([x1, x2], [y1, y2])\n",
    "    \n",
    "def insert_ones(x):\n",
    "    x = np.concatenate((x, np.ones((len(x), 1))), axis=1)\n",
    "    return x\n",
    "\n",
    "def predict(x, w):\n",
    "    return (np.dot(x, w)>0)*2-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_trials = 1000\n",
    "nb_iters = []\n",
    "e_ins = []\n",
    "e_outs = []\n",
    "nb_iters = []\n",
    "nb_train = 100\n",
    "nb_test = 1000\n",
    "for i in range(nb_trials):\n",
    "    x, y = generate(nb=nb_train + nb_test)\n",
    "    x = insert_ones(x)\n",
    "    x_train, y_train = x[0:nb_train], y[0:nb_train]\n",
    "    x_test, y_test = x[nb_train:], y[nb_train:]\n",
    "    w = lib.linreg(x_train, y_train)\n",
    "    \n",
    "    y_pred = predict(x_train, w)\n",
    "    e_in = (y_pred != y_train).mean()\n",
    "    e_ins.append(e_in)\n",
    "    \n",
    "    y_pred = predict(x_test, w)\n",
    "    e_out = (y_pred != y_test).mean()\n",
    "    \n",
    "    e_outs.append(e_out)\n",
    "        \n",
    "    x_train_perceptron, y_train_perceptron = x_train[0:10], y_train[0:10]\n",
    "    max_iter = 10000\n",
    "    w_perceptron, nb_iter = lib.run_perceptron(x_train_perceptron, y_train_perceptron, w=w.copy(), max_iter=max_iter)\n",
    "    assert nb_iter < max_iter\n",
    "    nb_iters.append(nb_iter)\n",
    "    \n",
    "print('q5', np.mean(e_ins))\n",
    "print('q6', np.mean(e_outs))\n",
    "print('q7', np.mean(nb_iters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 : closest is 0.01\n",
    "## Q6 : closest is 0.01\n",
    "## Q7 :  closest is 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate2(nb=100, noise=0):\n",
    "    x = np.random.uniform(-1, 1, size=(nb, 2))\n",
    "    y = ((x[:, 0]**2 + x[:, 1]**2 - 0.6)>0) * 2 - 1\n",
    "    if noise:\n",
    "        subset = np.arange(len(y))\n",
    "        np.random.shuffle(subset)\n",
    "        subset = subset[0:len(y)*noise]\n",
    "        y_noise = y.copy()\n",
    "        y_noise[subset] = -y_noise[subset]\n",
    "    else:\n",
    "        y_noise = y\n",
    "    return x, y, y_noise\n",
    "\n",
    "def augment(x):\n",
    "    x_feat = np.empty((len(x), 6))\n",
    "    x_feat[:, 0] = 1\n",
    "    x_feat[:, 1] = x[:, 0]\n",
    "    x_feat[:, 2] = x[:, 1]\n",
    "    x_feat[:, 3] = x[:, 0] * x[:, 1]\n",
    "    x_feat[:, 4] = x[:, 0]**2\n",
    "    x_feat[:, 5] = x[:, 1]**2\n",
    "    return x_feat\n",
    "\n",
    "g1 = [-1, -0.05, 0.08, 0.13, 1.5, 1.5]\n",
    "g2 = [-1, -0.05, 0.08, 0.13, 1.5, 1.5]\n",
    "g3 = [-1, -0.05, 0.08, 0.13, 15, 1.5]\n",
    "g4 = [-1, -1.5, 0.08, 0.13, 0.05, 0.05]\n",
    "g5 = [-1, -0.05, 0.08, 1.5, 0.15, 0.15]\n",
    "G = [g1, g2, g3 ,g4, g5]\n",
    "\n",
    "nb_trials = 1000\n",
    "e_ins = []\n",
    "e_ins_nonlinear = []\n",
    "e_outs = []\n",
    "count = defaultdict(int)\n",
    "for _ in range(nb_trials):\n",
    "    \n",
    "    # generate data\n",
    "    x_train, y_train, y_train_noise = generate2(nb=1000, noise=0.1)\n",
    "    x_train_features = augment(x_train)\n",
    "    x_train_features_linear = x_train_features[:, 0:3].copy()\n",
    "    x_test, y_test, y_test_noise = generate2(nb=1000, noise=0.1)\n",
    "    x_test_features = augment(x_test)\n",
    "\n",
    "    # E_in linear regression\n",
    "    w1 = lib.linreg(x_train_features_linear, y_train_noise)\n",
    "    y_pred = predict(x_train_features_linear, w1)\n",
    "    e_in = (y_pred != y_train_noise).mean()\n",
    "    e_ins.append(e_in)\n",
    "    # E_in linear regression with nonlinear features\n",
    "    w2 = lib.linreg(x_train_features, y_train_noise)    \n",
    "    y_pred = predict(x_train_features, w2)\n",
    "    e_in = (y_pred != y_train_noise).mean()\n",
    "    e_ins_nonlinear.append(e_in)\n",
    "    \n",
    "    y_pred_test = predict(x_test_features, w2)\n",
    "    e_out = (y_pred_test != y_test_noise).mean()\n",
    "    e_outs.append(e_out)\n",
    "    \n",
    "    # get closest hypothesis from G\n",
    "    E = []\n",
    "    for g in G:\n",
    "        y_pred = predict(x_test_features, g)\n",
    "        e = (y_pred_test != y_pred).mean()\n",
    "        E.append(e)\n",
    "    count[np.argmin(E)] += 1\n",
    "print('e_in', np.mean(e_ins))\n",
    "print('e_in non linear', np.mean(e_ins_nonlinear))\n",
    "print('e_out non linear', np.mean(e_outs))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8 : closest is 0.5\n",
    "## Q9 : closest is answer a\n",
    "## Q10 :closest answer is 0.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
