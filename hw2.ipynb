{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython import display\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "def linreg(X, y):\n",
    "    return np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n",
    "def run_perceptron(x_train, y_train, w=None, max_iter=100):\n",
    "    if w is None:\n",
    "        w = np.zeros((3,))\n",
    "    nb_updates = 0\n",
    "    for cur_iter in range(max_iter):\n",
    "        y_pred = (np.dot(x_train, w) > 0) * 2 - 1\n",
    "        ind = np.arange(len(y_train))[y_pred != y_train]\n",
    "        if len(ind) == 0:\n",
    "            break\n",
    "        i = np.random.choice(ind)\n",
    "        w += x_train[i] * y_train[i]\n",
    "        nb_updates += 1\n",
    "    return w, nb_updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# complexity of the linear model learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ is  $(N, p)$\n",
    "\n",
    "$y$ is $(N,)$\n",
    "\n",
    "$X^T X$ $(N,p)x(p, N)$ which has complexity $O(N^2p)$ (for loop over N, for loop over N, for loop over p)\n",
    "\n",
    "$(X^T X)^{-1}$ is $O(N^3)$\n",
    "\n",
    "$(X^T X)^{-1} y$ is $O(N^2)$\n",
    "\n",
    "total : \n",
    "\n",
    "$O(N^2p) + O(N^{3}) + O(N^{2})$\n",
    "\n",
    "Source : <https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Q1& Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = 0.5\n",
    "nb_coins = 1000\n",
    "nb_times = 10\n",
    "nb_repeats = 100000\n",
    "samples = np.random.uniform(size=(nb_repeats, nb_coins, nb_times)) <= mu\n",
    "s = samples.mean(axis=2)\n",
    "v1 = s[:, 0]\n",
    "vrand = s[np.arange(nb_repeats), np.random.randint(0, nb_coins, size=nb_repeats)]\n",
    "vmin = s.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.499623\n",
      "0.037668\n",
      "0.499812\n"
     ]
    }
   ],
   "source": [
    "print(vrand.mean())\n",
    "print(vmin.mean())\n",
    "print(v1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 :  closest is b\n",
    "\n",
    "## Q2 : c1 and crand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "modelisation of the problem\n",
    "\n",
    "- $h$ is a random variable denoting our hypothesis function.\n",
    "- $f$ is a random variable denoting our real function.\n",
    "- $f'$ is a random variable denoting the noisified version of the real function\n",
    "\n",
    "We have:\n",
    "\n",
    "- $P(h=1|f=1) = 1 - \\mu$\n",
    "- $P(h=0|f=0) = 1 - \\mu$\n",
    "- $P(f'=1|f=1) = \\lambda$\n",
    "- $P(f'=0|f=0) = \\lambda$\n",
    "\n",
    "we want to find : $P(h \\neq f') = P(h=1, f'=0) + P(h=1, f'=1)$\n",
    "\n",
    "just to verify for $P(h \\neq f), which is equal to $\\mu$\n",
    "\n",
    "$P(h \\neq f) = P(h=1, f=0) + P(h=0, f=1) = P(h=1|f=0)P(f=0) + P(h=0|f=1)P(f=1) = \\frac{1}{2}\\mu + \\frac{1}{2}\\mu = \\mu$\n",
    "\n",
    "now $P(h \\neq f')$.\n",
    "\n",
    "\n",
    "$P(h \\neq f') = P(h=1, f'=0) + P(h=0, f'=1)$\n",
    "\n",
    "$= P(h=1, f'=0|f=0)P(f=0) + P(h=1, f'=0|f=1)P(f=1) + P(h=0, f'=1|f=0)P(f=0) +  P(h=0, f'=1|f=1)P(f=1) $\n",
    "\n",
    "$= P(h=1|f=0)P(f'=0|f=0)P(f=0)+ P(h=1|f=1)P(f'=0|f=1)P(f=1)+ P(h=0|f=0)P(f'=1|f=0)P(f=1) + P(h=0|f=1)P(f'=1|f=1)P(f=1)$\n",
    "\n",
    "$ = \\frac{1}{2}\\mu \\lambda + \\frac{1}{2}(1-\\mu)(1-\\lambda) + \\frac{1}{2}\\mu \\lambda + \\frac{1}{2})(1-\\mu)(1-\\lambda)$\n",
    "\n",
    "$= \\mu \\lambda + (1-\\mu)(1-\\lambda)$\n",
    "\n",
    "## Q4\n",
    "\n",
    "well if $\\lambda = 0.5$ then $\\mu$ cancels, b is the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29913\n",
      "0.89948\n",
      "(0.66029000000000004, 0.66)\n"
     ]
    }
   ],
   "source": [
    "## q3 verification\n",
    "size = 100000\n",
    "x = np.random.uniform(size=size) <= 0.6\n",
    "f = (x > 0.4)\n",
    "\n",
    "mu = 0.3\n",
    "r = np.random.uniform(size=size) <= (1 - mu)\n",
    "h = f * r + (1 - f) * (1 - r)\n",
    "\n",
    "print((h!=f).mean())\n",
    "\n",
    "lambda_ = 0.1\n",
    "r = np.random.uniform(size=size)<= (lambda_)\n",
    "fprime = f * r + (1 - f) * (1 - r)\n",
    "print((fprime != f).mean())\n",
    "print((fprime != h).mean(), mu*lambda_ + (1-mu)*(1-lambda_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(nb=10):\n",
    "    x1, y1, x2, y2 = np.random.uniform(-1, 1, size=4)\n",
    "    m = (y2-y1)/(x2-x1)\n",
    "    p = y1 - m * x1\n",
    "    x_train = np.random.uniform(-1, 1, size=(nb, 2))\n",
    "    a = m\n",
    "    b = -1\n",
    "    c = p\n",
    "    w = np.array([a, b])\n",
    "    y_train = (np.dot(x_train, w) + c > 0) * 2 - 1\n",
    "    return x_train, y_train\n",
    "\n",
    "def viz(x_train, y_train, w, fig):\n",
    "    x1, x2 = x_train[:, 0].min(), x_train[:, 0].max()\n",
    "    y1, y2 = get_y(x1, x2, w[0], w[1], w[2])\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(x_train[:, 0], x_train[:, 1], c=y_train)\n",
    "    plt.plot([x1, x2], [y1, y2])\n",
    "    \n",
    "def get_y(x1, x2, a, b, c):\n",
    "    y1 = (- c - a * x1) / b\n",
    "    y2 = (- c - a * x2) / b\n",
    "    return y1, y2\n",
    "def insert_ones(x):\n",
    "    x = np.concatenate((x, np.ones((len(x), 1))), axis=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('q5', 0.038670000000000003)\n",
      "('q6', 0.047786000000000009)\n",
      "('q7', 3.9470000000000001)\n",
      "(281, 0)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMBAkSY2BIEkCDARJUmMgSJIA+H9rrrFPf1dvLAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efca441e4d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_trials = 1000\n",
    "nb_iters = []\n",
    "e_ins = []\n",
    "e_outs = []\n",
    "nb_iters = []\n",
    "nb_train = 100\n",
    "nb_test = 1000\n",
    "for i in range(nb_trials):\n",
    "    x, y = generate(nb=nb_train + nb_test)\n",
    "    x = insert_ones(x)\n",
    "    x_train, y_train = x[0:nb_train], y[0:nb_train]\n",
    "    x_test, y_test = x[nb_train:], y[nb_train:]\n",
    "    w = linreg(x_train, y_train)\n",
    "    \n",
    "    y_pred = predict(x_train, w)\n",
    "    e_in = (y_pred != y_train).mean()\n",
    "    e_ins.append(e_in)\n",
    "    \n",
    "    y_pred = predict(x_test, w)\n",
    "    e_out = (y_pred != y_test).mean()\n",
    "    \n",
    "    e_outs.append(e_out)\n",
    "        \n",
    "    x_train_perceptron, y_train_perceptron = x_train[0:10], y_train[0:10]\n",
    "    #x_train_perceptron = insert_ones(x_train_perceptron)\n",
    "    max_iter = 10000\n",
    "    w_perceptron, nb_iter = run_perceptron(x_train_perceptron, y_train_perceptron, w=w.copy(), max_iter=max_iter)\n",
    "    assert nb_iter < max_iter\n",
    "    nb_iters.append(nb_iter)\n",
    "    \n",
    "print('q5', np.mean(e_ins))\n",
    "print('q6', np.mean(e_outs))\n",
    "print('q7', np.mean(nb_iters))\n",
    "\n",
    "plt.hist(nb_iters)\n",
    "print(np.max(nb_iters), np.min(nb_iters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 : closest is 0.01\n",
    "## Q6 : closest is 0.01\n",
    "## Q7 :  closest is 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda2/envs/python2/lib/python2.7/site-packages/ipykernel/__main__.py:8: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e_in', 0.50444299999999997)\n",
      "('e_in non linear', 0.12374)\n",
      "('e_out non linear', 0.12598200000000001)\n",
      "defaultdict(<type 'int'>, {0: 1000})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "def generate2(nb=100, noise=0):\n",
    "    x = np.random.uniform(-1, 1, size=(nb, 2))\n",
    "    y = ((x[:, 0]**2 + x[:, 1]**2 - 0.6)>0) * 2 - 1\n",
    "    if noise:\n",
    "        subset = np.arange(len(y))\n",
    "        np.random.shuffle(subset)\n",
    "        subset = subset[0:len(y)*noise]\n",
    "        y_noise = y.copy()\n",
    "        y_noise[subset] = -y_noise[subset]\n",
    "    else:\n",
    "        y_noise = y\n",
    "    return x, y, y_noise\n",
    "\n",
    "def predict(x, w):\n",
    "    return (np.dot(x, w)>0) * 2 - 1\n",
    "\n",
    "def augment(x):\n",
    "    x_feat = np.empty((len(x), 6))\n",
    "    x_feat[:, 0] = 1\n",
    "    x_feat[:, 1] = x[:, 0]\n",
    "    x_feat[:, 2] = x[:, 1]\n",
    "    x_feat[:, 3] = x[:, 0] * x[:, 1]\n",
    "    x_feat[:, 4] = x[:, 0]**2\n",
    "    x_feat[:, 5] = x[:, 1]**2\n",
    "    return x_feat\n",
    "\n",
    "g1 = [-1, -0.05, 0.08, 0.13, 1.5, 1.5]\n",
    "g2 = [-1, -0.05, 0.08, 0.13, 1.5, 1.5]\n",
    "g3 = [-1, -0.05, 0.08, 0.13, 15, 1.5]\n",
    "g4 = [-1, -1.5, 0.08, 0.13, 0.05, 0.05]\n",
    "g5 = [-1, -0.05, 0.08, 1.5, 0.15, 0.15]\n",
    "G = [g1, g2, g3 ,g4, g5]\n",
    "nb_trials = 1000\n",
    "e_ins = []\n",
    "e_ins_nonlinear = []\n",
    "e_outs = []\n",
    "count = defaultdict(int)\n",
    "for _ in range(nb_trials):\n",
    "    \n",
    "    # generate data\n",
    "    x_train, y_train, y_train_noise = generate2(nb=1000, noise=0.1)\n",
    "    x_train_features = augment(x_train)\n",
    "    x_train_features_linear = x_train_features[:, 0:3].copy()\n",
    "    x_test, y_test, y_test_noise = generate2(nb=1000, noise=0.1)\n",
    "    x_test_features = augment(x_test)\n",
    "    \n",
    "\n",
    "    # E_in linear regression\n",
    "    w1 = linreg(x_train_features_linear, y_train_noise)\n",
    "    y_pred = predict(x_train_features_linear, w1)\n",
    "    e_in = (y_pred != y_train_noise).mean()\n",
    "    e_ins.append(e_in)\n",
    "    # E_in linear regression with nonlinear features\n",
    "    w2 = linreg(x_train_features, y_train_noise)    \n",
    "    y_pred = predict(x_train_features, w2)\n",
    "    e_in = (y_pred != y_train_noise).mean()\n",
    "    e_ins_nonlinear.append(e_in)\n",
    "    \n",
    "    y_pred_test = predict(x_test_features, w2)\n",
    "    e_out = (y_pred_test != y_test_noise).mean()\n",
    "    e_outs.append(e_out)\n",
    "    \n",
    "    # get closest hypothesis from G\n",
    "    E = []\n",
    "    for g in G:\n",
    "        y_pred = predict(x_test_features, g)\n",
    "        e = (y_pred_test != y_pred).mean()\n",
    "        E.append(e)\n",
    "    count[np.argmin(E)] += 1\n",
    "print('e_in', np.mean(e_ins))\n",
    "print('e_in non linear', np.mean(e_ins_nonlinear))\n",
    "print('e_out non linear', np.mean(e_outs))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8 : closest is 0.5\n",
    "## Q9 : closest is answer a\n",
    "## Q10 :closest answer is 0.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
